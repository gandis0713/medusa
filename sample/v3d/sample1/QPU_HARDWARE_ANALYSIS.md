# V3D 7.1 QPU 하드웨어 명령 실행 분석

## 목차
1. [QPU 하드웨어 구조](#qpu-하드웨어-구조)
2. [메모리 아키텍처](#메모리-아키텍처)
3. [명령어 실행 흐름](#명령어-실행-흐름)
4. [16-way SIMD 병렬 처리](#16-way-simd-병렬-처리)
5. [메모리 할당 및 관리](#메모리-할당-및-관리)
6. [TMU 동작 원리](#tmu-동작-원리)

---

## QPU 하드웨어 구조

### V3D 7.1 GPU 개요
```
┌────────────────────────────────────────────────────────────┐
│                    Raspberry Pi 5 SoC                      │
│  ┌────────────────────────────────────────────────────┐    │
│  │              V3D 7.1.6 GPU                         │    │
│  │  ┌─────────────────────────────────────────────┐   │    │
│  │  │            QPU (Quad Processing Unit)       │   │    │
│  │  │                                             │   │    │
│  │  │   ┌────────────────────────────────────┐    │   │    │
│  │  │   │  16-way SIMD Execution Engine      │    │   │    │
│  │  │   │  (16 threads execute in parallel)  │    │   │    │
│  │  │   └────────────────────────────────────┘    │   │    │
│  │  │                                             │   │    │
│  │  │   ┌─────────────┐    ┌──────────────┐       │   │    │
│  │  │   │   ALU Unit  │    │   MUL Unit   │       │   │    │
│  │  │   │  (ADD/SUB)  │    │   (FMUL)     │       │   │    │
│  │  │   └─────────────┘    └──────────────┘       │   │    │
│  │  │                                             │   │    │
│  │  │   ┌─────────────────────────────────────┐   │   │    │
│  │  │   │  Register File (rf0 ~ rf63)         │   │   │    │
│  │  │   │  Each register: 16 x 32-bit vector  │   │   │    │
│  │  │   └─────────────────────────────────────┘   │   │    │
│  │  │                                             │   │    │
│  │  │   ┌─────────────────────────────────────┐   │   │    │
│  │  │   │  TMU Interface Registers            │   │   │    │
│  │  │   │  - TMUA, TMUD, TMUWT                │   │   │    │
│  │  │   │  - 4 QPUs share 1 TMU (per Slice)   │   │   │    │
│  │  │   └─────────────────────────────────────┘   │   │    │
│  │  └─────────────────────────────────────────────┘   │    │
│  │                                                    │    │
│  │  ┌─────────────────────────────────────────────┐   │    │
│  │  │  TMU (Texture Memory Unit) - Shared         │   │    │
│  │  │  - Memory read/write pipeline               │   │    │
│  │  │  - Cache management (L1, L2)                │   │    │
│  │  │  - Serves 4 QPUs per Slice                  │   │    │
│  │  └─────────────────────────────────────────────┘   │    │
│  │                                                    │    │
│  │  Raspberry Pi 5: 12 QPUs, 3 Slices, 3 TMUs         │    │
│  │  (4 QPUs share 1 TMU per Slice)                    │    │
│  └────────────────────────────────────────────────────┘    │
└────────────────────────────────────────────────────────────┘
```

### QPU 핵심 특징

1. **16-way SIMD 아키텍처**
   - 하나의 명령어가 16개의 데이터에 동시 실행
   - 각 쓰레드는 동일한 명령어를 서로 다른 데이터에 실행 (SIMD)

2. **Register File**
   - 64개의 범용 레지스터 (rf0 ~ rf63)
   - 각 레지스터는 16개의 32-bit 값을 저장하는 벡터
   - 예: `rf7`은 실제로 `rf7[0], rf7[1], ..., rf7[15]`

3. **Dual-Issue 실행 유닛**
   - ADD/SUB 유닛과 MUL 유닛이 동시에 실행 가능
   - 하나의 명령어에서 두 개의 연산을 병렬 수행

---

## 메모리 아키텍처

```
┌────────────────────────────────────────────────────────────┐
│                      System Memory (DRAM)                  │
│                                                            │
│  ┌──────────────────────────────────────────────────────┐  │
│  │              GPU Virtual Address Space               │  │
│  │                                                      │  │
│  │  ┌─────────────────┐  ┌─────────────────┐            │  │
│  │  │   input_a BO    │  │   input_b BO    │            │  │
│  │  │  0x011a9000     │  │  0x011ad000     │            │  │
│  │  │  [64 bytes]     │  │  [64 bytes]     │            │  │
│  │  │                 │  │                 │            │  │
│  │  │  1  2  3  4     │  │  10 20 30 40    │            │  │
│  │  │  5  6  7  8     │  │  50 60 70 80    │            │  │
│  │  │  9 10 11 12     │  │  90 100 110 120 │            │  │
│  │  │ 13 14 15 16     │  │  130 140 150 160│            │  │
│  │  └─────────────────┘  └─────────────────┘            │  │
│  │                                                      │  │
│  │  ┌─────────────────┐  ┌─────────────────┐            │  │
│  │  │   output BO     │  │  QPU code BO    │            │  │
│  │  │  0x011b1000     │  │  0x011f9000     │            │  │
│  │  │  [64 bytes]     │  │  [192 bytes]    │            │  │
│  │  │                 │  │  (24 instrs)    │            │  │
│  │  │ 11 22 33 44     │  │                 │            │  │
│  │  │ 55 66 77 88     │  │  0x39800000...  │            │  │
│  │  │ 99 110 121 132  │  │  0x39804000...  │            │  │
│  │  │143 154 165 176  │  │  ...            │            │  │
│  │  └─────────────────┘  └─────────────────┘            │  │
│  │                                                      │  │
│  │  ┌─────────────────┐                                 │  │
│  │  │  Uniforms BO    │                                 │  │
│  │  │  0x01c19000     │                                 │  │
│  │  │  [12 bytes]     │                                 │  │
│  │  │                 │                                 │  │
│  │  │ [0]: 0x011a9000 │ ← input_a base                  │  │
│  │  │ [1]: 0x011ad000 │ ← input_b base                  │  │
│  │  │ [2]: 0x011b1000 │ ← output base                   │  │
│  │  └─────────────────┘                                 │  │
│  └──────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────┘
              ▲                           │
              │                           │ TMU access
              │ DRM ioctl                 ▼
┌─────────────┴─────────────────────────────────────────────┐
│                     Linux Kernel                          │
│  ┌──────────────────────────────────────────────────────┐ │
│  │             V3D DRM Driver                           │ │
│  │  - Buffer Object (BO) management                     │ │
│  │  - GPU memory allocation (DRM_IOCTL_V3D_CREATE_BO)   │ │
│  │  - Memory mapping (DRM_IOCTL_V3D_MMAP_BO)            │ │
│  │  - Command submission (DRM_IOCTL_V3D_SUBMIT_CSD)     │ │
│  └──────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────┘
```

### Buffer Object (BO) 관리

각 BO는 다음 정보를 포함:
- **handle**: DRM 드라이버 내부 식별자
- **offset**: GPU 가상 주소 (실제 물리 주소와 매핑)
- **size**: 버퍼 크기 (바이트)
- **map**: CPU에서 접근 가능한 가상 주소

---

## 명령어 실행 흐름

### 프로그램 구조 (21개 명령어)

```
명령어 번호   │  어셈블리                          │  설명
─────────────┼────────────────────────────────────┼──────────────────────
0            │  ldunifrf.rf0                      │  uniform[0] → rf0 (input_a base)
1            │  ldunifrf.rf1                      │  uniform[1] → rf1 (input_b base)
2            │  ldunifrf.rf2                      │  uniform[2] → rf2 (output base)
3            │  eidx rf7                          │  EIDX → rf7 (thread ID: 0~15)
4            │  add rf8, rf7, rf7                 │  rf7 * 2 → rf8
5            │  add rf9, rf8, rf8                 │  rf8 * 2 → rf9 (thread_id * 4)
6            │  add rf3, rf0, rf9                 │  input_a base + offset → rf3
7            │  add rf4, rf1, rf9                 │  input_b base + offset → rf4
8            │  add rf5, rf2, rf9                 │  output base + offset → rf5
9            │  mov tmua, rf3                     │  TMU read request: input_a[tid]
10           │  mov tmua, rf4                     │  TMU read request: input_b[tid]
11           │  nop                               │  TMU latency
12           │  ldtmu.rf10                        │  TMU response → rf10 (input_a[tid])
13           │  ldtmu.rf11                        │  TMU response → rf11 (input_b[tid])
14           │  add rf12, rf10, rf11              │  rf10 + rf11 → rf12
15           │  mov tmud, rf12                    │  Prepare write data
16           │  mov tmua, rf5                     │  TMU write: output[tid] = rf12
17           │  tmuwt rf0                         │  Wait for TMU writes to complete
18           │  nop                               │  Post-tmuwt delay
19           │  thrsw                             │  Thread switch (prepare to exit)
20           │  nop                               │  Thrsw delay slot
```

### 실제 16진수 명령어

```
 0: 0x39800000bb03f000   ldunifrf.rf0
 1: 0x39804000bb03f000   ldunifrf.rf1
 2: 0x39808000bb03f000   ldunifrf.rf2
 3: 0x38000007bb03f002   eidx rf7
 4: 0x380000083803f1c7   add rf8, rf7, rf7
 5: 0x380000093803f208   add rf9, rf8, rf8
 6: 0x380000033803f009   add rf3, rf0, rf9
 7: 0x380000043803f049   add rf4, rf1, rf9
 8: 0x380000053803f089   add rf5, rf2, rf9
 9: 0x3800100cf903f0c3   mov tmua, rf3
10: 0x3800100cf903f103   mov tmua, rf4
11: 0x38000000bb03f000   nop
12: 0x38828000bb03f000   ldtmu.rf10
13: 0x3882c000bb03f000   ldtmu.rf11
14: 0x3800000c3803f28b   add rf12, rf10, rf11
15: 0x3800100bf903f303   mov tmud, rf12
16: 0x3800100cf903f143   mov tmua, rf5
17: 0x38000000bb03f00f   tmuwt rf0
18: 0x38000000bb03f000   nop
19: 0x38200000bb03f000   thrsw
20: 0x38000000bb03f000   nop
```

---

## 16-way SIMD 병렬 처리

### EIDX (Element Index) 동작

```
명령어 3: eidx rf7
─────────────────────────────────────────────────────

실행 전: rf7 = [undefined × 16]

실행 후: rf7 = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                │  │  │  │  │  │  │  │  │  │   │   │   │   │   │   └─ Thread 15
                │  │  │  │  │  │  │  │  │  │   │   │   │   │   └───── Thread 14
                │  │  │  │  │  │  │  │  │  │   │   │   │   └───────── Thread 13
                │  │  │  │  │  │  │  │  │  │   │   │   └───────────── ...
                │  │  │  │  │  │  │  │  │  │   │   └───────────────── Thread 2
                │  │  │  │  │  │  │  │  │  │   └───────────────────── Thread 1
                └──┴──┴──┴──┴──┴──┴──┴──┴──┴───────────────────────── Thread 0

각 쓰레드는 자신의 고유한 ID를 얻습니다.
```

### 오프셋 계산 (명령어 4-5)

```
명령어 4: add rf8, rf7, rf7    (rf7 * 2)
명령어 5: add rf9, rf8, rf8    (rf8 * 2 = rf7 * 4)
────────────────────────────────────────────────────────

Thread ID (rf7):  [0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]
                   │   │   │   │   │   │   │   │   │   │   │   │   │   │   │   │
                   ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2
                   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓
rf7 * 2 (rf8):    [0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30]
                   │   │   │   │   │   │   │   │   │   │   │   │   │   │   │   │
                   ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2  ×2
                   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓
rf7 * 4 (rf9):    [0,  4,  8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60]
                   │   │   │   │   │   │   │   │   │   │   │   │   │   │   │   │
                   └───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴──→ 바이트 오프셋
```

### 주소 계산 (명령어 6-8)

```
Uniform에서 로드한 베이스 주소:
  rf0 = 0x011a9000  (input_a base)
  rf1 = 0x011ad000  (input_b base)
  rf2 = 0x011b1000  (output base)

명령어 6: add rf3, rf0, rf9
──────────────────────────────────────────────────────────────────────
Thread  │ rf0          │ rf9    │ rf3 (input_a[tid] addr)  │ Value
────────┼──────────────┼────────┼──────────────────────────┼──────
   0    │ 0x011a9000   │    0   │ 0x011a9000               │  1
   1    │ 0x011a9000   │    4   │ 0x011a9004               │  2
   2    │ 0x011a9000   │    8   │ 0x011a9008               │  3
   3    │ 0x011a9000   │   12   │ 0x011a900c               │  4
   ...  │ ...          │  ...   │ ...                      │ ...
  15    │ 0x011a9000   │   60   │ 0x011a903c               │ 16

명령어 7: add rf4, rf1, rf9
──────────────────────────────────────────────────────────────────────
Thread  │ rf1          │ rf9    │ rf4 (input_b[tid] addr)  │ Value
────────┼──────────────┼────────┼──────────────────────────┼──────
   0    │ 0x011ad000   │    0   │ 0x011ad000               │  10
   1    │ 0x011ad000   │    4   │ 0x011ad004               │  20
   2    │ 0x011ad000   │    8   │ 0x011ad008               │  30
   3    │ 0x011ad000   │   12   │ 0x011ad00c               │  40
   ...  │ ...          │  ...   │ ...                      │ ...
  15    │ 0x011ad000   │   60   │ 0x011ad03c               │ 160

명령어 8: add rf5, rf2, rf9
──────────────────────────────────────────────────────────────────────
Thread  │ rf2          │ rf9    │ rf5 (output[tid] addr)
────────┼──────────────┼────────┼──────────────────────────
   0    │ 0x011b1000   │    0   │ 0x011b1000
   1    │ 0x011b1000   │    4   │ 0x011b1004
   2    │ 0x011b1000   │    8   │ 0x011b1008
   3    │ 0x011b1000   │   12   │ 0x011b100c
   ...  │ ...          │  ...   │ ...
  15    │ 0x011b1000   │   60   │ 0x011b103c
```

### 병렬 연산 실행

```
명령어 14: add rf12, rf10, rf11
─────────────────────────────────────────────────────────────────

Thread  │  rf10      │  rf11     │  rf12 (result)  │  설명
────────┼────────────┼───────────┼─────────────────┼───────────────────
   0    │     1      │    10     │      11         │  input_a[0] + input_b[0]
   1    │     2      │    20     │      22         │  input_a[1] + input_b[1]
   2    │     3      │    30     │      33         │  input_a[2] + input_b[2]
   3    │     4      │    40     │      44         │  input_a[3] + input_b[3]
   4    │     5      │    50     │      55         │  input_a[4] + input_b[4]
   5    │     6      │    60     │      66         │  input_a[5] + input_b[5]
   6    │     7      │    70     │      77         │  input_a[6] + input_b[6]
   7    │     8      │    80     │      88         │  input_a[7] + input_b[7]
   8    │     9      │    90     │      99         │  input_a[8] + input_b[8]
   9    │    10      │   100     │     110         │  input_a[9] + input_b[9]
  10    │    11      │   110     │     121         │  input_a[10] + input_b[10]
  11    │    12      │   120     │     132         │  input_a[11] + input_b[11]
  12    │    13      │   130     │     143         │  input_a[12] + input_b[12]
  13    │    14      │   140     │     154         │  input_a[13] + input_b[13]
  14    │    15      │   150     │     165         │  input_a[14] + input_b[14]
  15    │    16      │   160     │     176         │  input_a[15] + input_b[15]

⚡ 16개의 덧셈이 단일 클럭 사이클에 병렬로 실행됩니다!
```

---

## 메모리 할당 및 관리

### 1. Buffer Object 생성

```c
// DRM ioctl을 통한 GPU 메모리 할당
struct drm_v3d_create_bo create_bo = {
    .size = size,
    .flags = 0
};
ioctl(drm_fd, DRM_IOCTL_V3D_CREATE_BO, &create_bo);

// 결과:
// - handle: DRM 내부 식별자
// - offset: GPU 가상 주소 (0x011a9000 같은 형태)
```

**메모리 할당 과정:**

```
┌─────────────┐
│ User Space  │
│   (CPU)     │
└──────┬──────┘
       │ DRM_IOCTL_V3D_CREATE_BO
       │ size = 64 bytes
       ▼
┌─────────────────────────────────────┐
│   Linux Kernel                      │
│  ┌──────────────────────────────┐   │
│  │  V3D DRM Driver              │   │
│  │  1. Allocate physical memory │   │
│  │  2. Create GPU page table    │   │
│  │  3. Map to GPU virtual addr  │   │
│  │  4. Return handle & offset   │   │
│  └──────────────────────────────┘   │
└─────────────────────────────────────┘
       │
       │ Returns: handle=1, offset=0x011a9000
       ▼
┌─────────────┐
│ User Space  │
│   (CPU)     │
└─────────────┘
```

### 2. CPU 접근을 위한 메모리 매핑

```c
// GPU 메모리를 CPU 주소 공간에 매핑
struct drm_v3d_mmap_bo mmap_bo = {
    .handle = handle,
    .flags = 0
};
ioctl(drm_fd, DRM_IOCTL_V3D_MMAP_BO, &mmap_bo);

void* cpu_addr = mmap(NULL, size, PROT_READ | PROT_WRITE,
                      MAP_SHARED, drm_fd, mmap_bo.offset);
```

**메모리 매핑 과정:**

```
GPU Virtual Address Space          Physical Memory          CPU Virtual Address Space
┌──────────────────────┐           ┌──────────────┐        ┌──────────────────────┐
│                      │           │              │        │                      │
│  0x011a9000          │◄──────────┤ Page Frame 1 ├───────►│  0x7f8a12340000     │
│  [64 bytes]          │   GPU     │  [64 bytes]  │  CPU   │  [64 bytes]          │
│  input_a BO          │   page    │              │  page  │  CPU mapping         │
│                      │   table   │              │  table │                      │
└──────────────────────┘           └──────────────┘        └──────────────────────┘
        ▲                                  ▲                         ▲
        │                                  │                         │
        └──────────────────────────────────┴─────────────────────────┘
              Same physical memory, different virtual addresses
```

### 3. 5개의 Buffer Objects

프로그램은 총 5개의 BO를 생성:

```
┌──────────────┬────────────┬──────────┬─────────────────────────────┐
│ BO Name      │ Size       │ GPU Addr │ Purpose                     │
├──────────────┼────────────┼──────────┼─────────────────────────────┤
│ code_bo      │ 168 bytes  │0x011f9000│ QPU 명령어 (21 × 8 bytes)   │
│ input_a_bo   │ 64 bytes   │0x011a9000│ 입력 벡터 A (16 × 4 bytes)  │
│ input_b_bo   │ 64 bytes   │0x011ad000│ 입력 벡터 B (16 × 4 bytes)  │
│ output_bo    │ 64 bytes   │0x011b1000│ 출력 벡터 (16 × 4 bytes)    │
│ uniforms_bo  │ 12 bytes   │0x01c19000│ Uniform 데이터 (3 × 4 bytes)│
└──────────────┴────────────┴──────────┴─────────────────────────────┘
```

### 4. Compute Shader Dispatch (CSD) 설정

```c
struct drm_v3d_submit_csd submit = {
    .cfg[0] = 0x00010000,  // wg_count_x = 1
    .cfg[1] = 0x00010000,  // wg_count_y = 1
    .cfg[2] = 0x00010000,  // wg_count_z = 1
    .cfg[3] = 0x00000110,  // wg_size = 16, wgs_per_sg = 1
    .cfg[4] = 0x00000001,  // num_batches = 1
    .cfg[5] = 0x011f9002,  // shader address + flags
    .cfg[6] = 0x01c19000,  // uniforms address

    .bo_handles = {code_handle, input_a_handle, input_b_handle,
                   output_handle, uniforms_handle},
    .bo_handle_count = 5,

    .perfmon_id = 0,
    .flags = 0
};

ioctl(drm_fd, DRM_IOCTL_V3D_SUBMIT_CSD, &submit);
```

**CSD 파라미터 설명:**

```
cfg[3] = 0x00000110 의 구조:
┌────────┬────────────┬─────────────────┬──────────┐
│ Bits   │   Field    │     Value       │ Meaning  │
├────────┼────────────┼─────────────────┼──────────┤
│  7:0   │ WG_SIZE    │ 0x10 (16)       │ 16 threads per workgroup │
│ 11:8   │ WGS_PER_SG │ 0x01 (1)        │ 1 workgroup per supergroup │
│ 19:12  │ BATCHES    │ 0x00 (0)        │ batches_per_sg - 1 = 0 │
│ 25:20  │ MAX_SG_ID  │ 0x00 (0)        │ Maximum supergroup ID │
└────────┴────────────┴─────────────────┴──────────┘

Workgroup configuration:
  - 1 workgroup (1×1×1)
  - 16 threads per workgroup
  - All threads execute in parallel (16-way SIMD)
```

### 5. 메모리 정리

```c
// BO 언매핑
munmap(cpu_addr, size);

// BO 삭제 (reference count 감소)
struct drm_gem_close gem_close = {
    .handle = handle
};
ioctl(drm_fd, DRM_IOCTL_GEM_CLOSE, &gem_close);

// DRM 디바이스 닫기
close(drm_fd);
```

---

## TMU 동작 원리

### TMU (Texture Memory Unit) 구조

```
┌─────────────────────────────────────────────────────────────┐
│                    QPU Core                                  │
│                                                              │
│  ┌─────────────┐         ┌──────────────────────┐          │
│  │  RF (rf0-63)│         │  TMU Control Regs    │          │
│  │             │         │                      │          │
│  │  rf3: addr ─┼────────►│  TMUA (Address)      │          │
│  │  rf10: data◄┼─────────┤  TMUD (Data)         │          │
│  │             │         │  TMUWT (Wait)        │          │
│  └─────────────┘         └──────────┬───────────┘          │
│                                     │                       │
└─────────────────────────────────────┼───────────────────────┘
                                      │
                                      ▼
              ┌──────────────────────────────────────┐
              │         TMU Pipeline                  │
              │                                       │
              │  ┌──────────┐    ┌──────────┐       │
              │  │ L1 Cache │    │ L2 Cache │       │
              │  │  (16 KB) │    │ (128 KB) │       │
              │  └────┬─────┘    └────┬─────┘       │
              │       │               │              │
              │       └───────┬───────┘              │
              │               ▼                      │
              │      ┌─────────────────┐            │
              │      │  Memory         │            │
              │      │  Controller     │            │
              │      └────────┬────────┘            │
              └───────────────┼─────────────────────┘
                              │
                              ▼
                      ┌───────────────┐
                      │  System DRAM  │
                      └───────────────┘
```

### TMU 읽기 동작 (명령어 9-13)

```
명령어 9:  mov tmua, rf3    (TMU read request for input_a)
명령어 10: mov tmua, rf4    (TMU read request for input_b)
명령어 11: nop              (TMU latency - 여러 클럭 필요)
명령어 12: ldtmu.rf10       (Get first response)
명령어 13: ldtmu.rf11       (Get second response)
```

**타임라인:**

```
Cycle │ Action                                      │ TMU State
──────┼─────────────────────────────────────────────┼──────────────────────
  0   │ mov tmua, rf3                               │ Request queue: [addr3]
      │   Thread 0: rf3 = 0x011a9000                │
      │   Thread 1: rf3 = 0x011a9004                │
      │   ...                                       │
      │   (16 addresses queued)                     │
──────┼─────────────────────────────────────────────┼──────────────────────
  1   │ mov tmua, rf4                               │ Request queue: [addr3, addr4]
      │   Thread 0: rf4 = 0x011ad000                │
      │   Thread 1: rf4 = 0x011ad004                │
      │   ...                                       │
      │   (16 more addresses queued)                │
──────┼─────────────────────────────────────────────┼──────────────────────
  2   │ nop                                         │ Processing requests...
      │                                             │ L1 cache lookup
      │                                             │ Possible L2 or DRAM access
──────┼─────────────────────────────────────────────┼──────────────────────
 3-10 │ (TMU is fetching data from memory)          │ Memory access in progress
      │                                             │ Response queue filling up
──────┼─────────────────────────────────────────────┼──────────────────────
  11  │ ldtmu.rf10                                  │ Response queue: [data3, data4]
      │   Thread 0: rf10 = 1                        │ Pop first 16 values
      │   Thread 1: rf10 = 2                        │ (from addr3 requests)
      │   ...                                       │
      │   Thread 15: rf10 = 16                      │
──────┼─────────────────────────────────────────────┼──────────────────────
  12  │ ldtmu.rf11                                  │ Response queue: [data4]
      │   Thread 0: rf11 = 10                       │ Pop second 16 values
      │   Thread 1: rf11 = 20                       │ (from addr4 requests)
      │   ...                                       │
      │   Thread 15: rf11 = 160                     │
──────┴─────────────────────────────────────────────┴──────────────────────
```

### TMU 쓰기 동작 (명령어 15-17)

```
명령어 15: mov tmud, rf12   (Prepare write data)
명령어 16: mov tmua, rf5    (Trigger write to address)
명령어 17: tmuwt rf0        (Wait for all writes to complete)
```

**중요: TMUD와 TMUA는 반드시 별도 명령어로!**

```
❌ 잘못된 방법 (동시에 쓰기):
┌────────────────────────────────────────┐
│  instr.alu.add: mov tmud, rf12         │
│  instr.alu.mul: mov tmua, rf5          │  ← GPU가 주소 0에 접근!
└────────────────────────────────────────┘

✅ 올바른 방법 (순차적으로 쓰기):
┌────────────────────────────────────────┐
│  명령어 15: mov tmud, rf12             │  ← 데이터 준비
└────────────────────────────────────────┘
┌────────────────────────────────────────┐
│  명령어 16: mov tmua, rf5              │  ← 주소 지정하면 쓰기 시작
└────────────────────────────────────────┘
```

**TMU 쓰기 타임라인:**

```
Cycle │ Action                                      │ TMU State
──────┼─────────────────────────────────────────────┼──────────────────────
  0   │ mov tmud, rf12                              │ Data buffer: [rf12 values]
      │   Thread 0: rf12 = 11                       │ (Stored in TMU)
      │   Thread 1: rf12 = 22                       │
      │   ...                                       │
──────┼─────────────────────────────────────────────┼──────────────────────
  1   │ mov tmua, rf5                               │ Write initiated:
      │   Thread 0: write 11 to 0x011b1000          │ - 16 writes queued
      │   Thread 1: write 22 to 0x011b1004          │ - L1 cache updated
      │   ...                                       │ - Writeback to DRAM pending
      │   Thread 15: write 176 to 0x011b103c        │
──────┼─────────────────────────────────────────────┼──────────────────────
 2-10 │ (TMU is writing data to memory)             │ Cache writeback
      │                                             │ DRAM update
──────┼─────────────────────────────────────────────┼──────────────────────
  11  │ tmuwt rf0                                   │ Wait for completion
      │   (Stalls until all writes finish)          │ All writes committed
──────┴─────────────────────────────────────────────┴──────────────────────
```

### TMU 성능 특성

1. **요청 큐잉**: 여러 TMU 요청을 큐에 넣고 한 번에 처리
2. **Cache 계층**: L1 (16KB) → L2 (128KB) → DRAM
3. **Coalescing**: 인접한 메모리 접근을 병합하여 대역폭 최적화
4. **레이턴시**:
   - L1 hit: ~5 cycles
   - L2 hit: ~20 cycles
   - DRAM: ~100+ cycles

---

## 전체 실행 플로우 요약

```
┌─────────────────────────────────────────────────────────────────┐
│ 1. 초기화 단계                                                   │
│    - DRM 디바이스 열기 (/dev/dri/renderD128)                    │
│    - 5개 BO 생성 및 매핑                                         │
│    - QPU 코드 생성 (24 명령어)                                   │
│    - 입력 데이터 초기화                                          │
└─────────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────────┐
│ 2. GPU 작업 제출                                                 │
│    - CSD 구성 설정                                               │
│    - DRM_IOCTL_V3D_SUBMIT_CSD                                   │
│    - Kernel이 GPU에 작업 스케줄링                                │
└─────────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────────┐
│ 3. GPU 실행 (QPU에서)                                            │
│    ┌─────────────────────────────────────────────────────────┐  │
│    │ Phase 1: Setup (명령어 0-2)                              │  │
│    │   - Uniform 로드 (베이스 주소들)                         │  │
│    └─────────────────────────────────────────────────────────┘  │
│    ┌─────────────────────────────────────────────────────────┐  │
│    │ Phase 2: Thread ID & Offset (명령어 3-5)                 │  │
│    │   - EIDX로 각 쓰레드 ID 획득                             │  │
│    │   - Thread ID × 4 계산                                   │  │
│    └─────────────────────────────────────────────────────────┘  │
│    ┌─────────────────────────────────────────────────────────┐  │
│    │ Phase 3: Address Calculation (명령어 6-8)                │  │
│    │   - 각 쓰레드의 메모리 주소 계산                         │  │
│    │   - Base + Offset                                        │  │
│    └─────────────────────────────────────────────────────────┘  │
│    ┌─────────────────────────────────────────────────────────┐  │
│    │ Phase 4: Memory Read (명령어 9-13)                       │  │
│    │   - TMU를 통한 input_a, input_b 읽기                     │  │
│    │   - 각 쓰레드가 자신의 요소 로드                         │  │
│    └─────────────────────────────────────────────────────────┘  │
│    ┌─────────────────────────────────────────────────────────┐  │
│    │ Phase 5: Computation (명령어 14)                         │  │
│    │   - 16개 쓰레드가 병렬로 덧셈 수행                       │  │
│    │   - 단일 클럭에 16개 ADD 동시 실행!                      │  │
│    └─────────────────────────────────────────────────────────┘  │
│    ┌─────────────────────────────────────────────────────────┐  │
│    │ Phase 6: Memory Write (명령어 15-17)                     │  │
│    │   - TMU를 통한 output 쓰기                               │  │
│    │   - 모든 쓰기 완료 대기 (tmuwt)                          │  │
│    └─────────────────────────────────────────────────────────┘  │
│    ┌─────────────────────────────────────────────────────────┐  │
│    │ Phase 7: Cleanup (명령어 18-20)                          │  │
│    │   - NOP 지연                                             │  │
│    │   - 쓰레드 스위치 (thrsw)                                │  │
│    │   - 프로그램 종료                                        │  │
│    └─────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────────┐
│ 4. 완료 대기                                                     │
│    - DRM_IOCTL_V3D_WAIT_BO                                      │
│    - GPU 실행 완료 신호 대기                                     │
└─────────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────────┐
│ 5. 결과 검증                                                     │
│    - CPU에서 output 버퍼 읽기                                    │
│    - 각 요소가 올바른지 확인                                     │
│    - output[i] == input_a[i] + input_b[i]                       │
└─────────────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────────────┐
│ 6. 정리                                                          │
│    - BO 언매핑 (munmap)                                          │
│    - BO 삭제 (DRM_IOCTL_GEM_CLOSE)                              │
│    - DRM 디바이스 닫기                                           │
└─────────────────────────────────────────────────────────────────┘
```

---

## 핵심 포인트 요약

### 1. SIMD의 힘
- **하나의 명령어로 16개의 데이터 처리**
- 명령어 15 (add)는 단일 클럭에 16개 덧셈 수행
- 전통적 CPU 루프보다 16배 효율적

### 2. EIDX의 중요성
- 각 쓰레드가 고유 ID를 얻는 유일한 방법
- EIDX 없이는 모든 쓰레드가 동일한 데이터 처리
- SIMD 프로그래밍의 핵심 요소

### 3. TMU 사용 규칙
- **TMUD와 TMUA는 반드시 별도 명령어로 분리**
- 동시 쓰기 시 GPU가 주소 0을 접근하는 버그
- ldtmu는 FIFO 순서로 응답 반환

### 4. 메모리 관리
- DRM ioctl을 통한 BO 관리
- GPU와 CPU 모두 접근 가능한 shared memory
- Offset은 GPU 가상 주소로 사용

### 5. 성능 특성
- **총 21개 명령어**로 16개 요소 처리
- 루프 언롤링 대신 SIMD 병렬 처리
- TMU 레이턴시를 숨기기 위한 명령어 배치

---

## 참고 자료

- **V3D GPU Architecture**: Broadcom VideoCore VII
- **Driver**: Linux kernel v3d DRM driver
- **ISA**: V3D 7.1 Instruction Set Architecture
- **Hardware**: Raspberry Pi 5 (BCM2712)

이 문서는 `qpu_simple_add.c` 프로그램의 하드웨어 레벨 동작을 설명합니다.
